{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import jpype\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation, digits\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "from typing import List\n",
    "from jpype import JClass, JString, getDefaultJVMPath, shutdownJVM, startJVM, java\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from string import punctuation, digits\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"son data\\FINAL-TAG(train).csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSİNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceemoticon(word):\n",
    "    \n",
    "    pos = re.findall(r\"(?::\\) | :-\\) | =\\) | :D | :d | <3 | \\(: | :\\'\\) | \\^\\^|;\\) | \\(-:)\", word)\n",
    "    neg = re.findall(r\"(:-\\( | :\\( | ;\\( | ;-\\( | =\\( | :/ | :\\\\ | -_- | \\): | \\)-:)\", word)\n",
    "  \n",
    "    if pos:\n",
    "        #word = \":)\"\n",
    "        word = word + \" pozitif\"\n",
    "\n",
    "    elif neg:\n",
    "        #word = \":(\"\n",
    "        word = word + \" negatif\"\n",
    "\n",
    "    return word\n",
    "\n",
    "def preProcessing(text):\n",
    "    return resubsatır(df_remove_punc(df_remove_digits(unlem(df_remove_email_adressed(df_remove_com_tr(df_remove_link(text)))))))\n",
    "\n",
    "def df_remove_link(text):\n",
    "    return re.sub(r\"((http\\S+)|(www\\S+))\", \" \", text)\n",
    "\n",
    "def df_remove_com_tr(text):\n",
    "    text = re.sub(r'\\S*\\.(com|tr)',\" \",text)\n",
    "    return text\n",
    "\n",
    "def df_remove_email_adressed(text):\n",
    "    return re.sub(r'([.\\w]{3,}@[.\\w]{5,})', ' ', text)\n",
    "\n",
    "def df_remove_digits(text):\n",
    "    text=text.strip()\n",
    "    remove_digits = str.maketrans(' ', ' ', digits)\n",
    "    return text.translate(remove_digits)\n",
    "\n",
    "def unlem(text):\n",
    "    if \"!\" in text:\n",
    "        return text+\" ünlem\"\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def resubsatır(text):\n",
    "    texts= re.sub(\"\\n\",\" \",text)\n",
    "    return texts\n",
    "\n",
    "def df_remove_punc(text):\n",
    "    regex = re.compile('[%s]' % re.escape(punctuation))\n",
    "    return regex.sub(' ', text)\n",
    "\n",
    "\n",
    "def replace_a(text):\n",
    "    if \"â\" in text:\n",
    "        return text+\"a\"\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sil_1(text):\n",
    "    text_list=text.split()\n",
    "    for i in text_list:\n",
    "        if len(i)==1:\n",
    "            text_list.remove(i)   \n",
    "    return \" \".join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WPT = nltk.WordPunctTokenizer()\n",
    "stop_word_list = nltk.corpus.stopwords.words('turkish')\n",
    "stop_word_list.remove('ama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_sil(text):\n",
    "    text_list=text.split()\n",
    "    silinmis_list = [text for text in text_list if text not in stop_word_list]\n",
    "    silinmis_list=\" \".join(silinmis_list)\n",
    "    ##print(silinmis_list)\n",
    "    return silinmis_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZEMBEREK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jpype1 in c:\\users\\eda\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jpype1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jpype\n",
    "from jpype import getDefaultJVMPath,JClass\n",
    "jar = r\"utilities/jar/zemberek-full.jar\"\n",
    "from jpype import getDefaultJVMPath,JClass\n",
    "if not jpype.isJVMStarted():\n",
    "    jpype.startJVM(getDefaultJVMPath(), classpath=[jar])\n",
    "\n",
    "ZEMBEREK_PATH = r'utilities/jar/zemberek-full.jar'\n",
    "TurkishMorphology = JClass('zemberek.morphology.TurkishMorphology')\n",
    "TurkishSpellChecker = JClass('zemberek.normalization.TurkishSpellChecker')\n",
    "TurkishSentenceNormalizer = JClass('zemberek.normalization.TurkishSentenceNormalizer')\n",
    "Paths = JClass('java.nio.file.Paths')\n",
    "lookupRoot = Paths.get(\"utilities/normalization\")\n",
    "lmPath = Paths.get(\"utilities/data/lm/lm.2gram.slm\")\n",
    "morphology = TurkishMorphology.createWithDefaults()\n",
    "morph = TurkishMorphology.createWithDefaults()\n",
    "spell = TurkishSpellChecker(morph)\n",
    "#normalizer = TurkishSentenceNormalizer(morphology, lookupRoot, lmPath)\n",
    "\n",
    "WPT = nltk.WordPunctTokenizer()\n",
    "stop_word_list = nltk.corpus.stopwords.words('turkish')\n",
    "stop_word_list.remove('ama')\n",
    "WORDS = dict()\n",
    "spell_folder = Path(r\"utilities/big2\")\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "with open(os.path.expanduser(Path(spell_folder/ \"big2_yesim.txt\")), \"r\", encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        splitted = line.split()\n",
    "        WORDS[splitted[0]] = int(splitted[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse(s): \n",
    "    if len(s) == 0: \n",
    "        return s \n",
    "    else: \n",
    "        return reverse(s[1:]) + s[0] \n",
    "\n",
    "def checkOpennes(word):\n",
    "    vowels=['a','e','i','ı','o','ö','u','ü']\n",
    "    open_vowels=['e','i','ü','ö']\n",
    "    close_vowels=['a','ı','o','u']\n",
    "    for i in range(len(word)):\n",
    "        if reverse(word)[i] in vowels:\n",
    "            if reverse(word)[i] in open_vowels:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            continue\n",
    "def PresentCheck(word):\n",
    "        ei=['e','i']\n",
    "        aı=['a','ı']\n",
    "        üö=['ü','ö']\n",
    "        uo=['u','o']\n",
    "        for i in range(len(word)):\n",
    "            if reverse(word)[i] in ei:\n",
    "                return 'ei'\n",
    "            elif reverse(word)[i] in üö:\n",
    "                return 'üö'\n",
    "            elif reverse(word)[i] in aı:\n",
    "                return 'aı'\n",
    "            elif reverse(word)[i] in uo:\n",
    "                return 'uo'\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "def StartCheck(word):\n",
    "        ei=['e','i']\n",
    "        aı=['a','ı']\n",
    "        üö=['ü','ö']\n",
    "        uo=['u','o']\n",
    "        for i in range(len(word)):\n",
    "            if (word)[i] in ei:\n",
    "                return 'ei'\n",
    "            elif (word)[i] in üö:\n",
    "                return 'üö'\n",
    "            elif (word)[i] in aı:\n",
    "                return 'aı'\n",
    "            elif (word)[i] in uo:\n",
    "                return 'uo'\n",
    "            else:\n",
    "                continue\n",
    "def replaceall(s, n,a):\n",
    "    occurence = s.count(n)\n",
    "    alt = []\n",
    "    temp = s\n",
    "    for i in range(occurence):\n",
    "        temp2 = temp\n",
    "        for j in range(i,occurence):\n",
    "            temp2 = temp2.replace(n,a,1)\n",
    "            alt.append(temp2)\n",
    "        temp = temp.replace(n,\"!\",1)\n",
    "    for i in range(len(alt)):\n",
    "        alt[i] = alt[i].replace(\"!\",n)\n",
    "\n",
    "    return alt\n",
    "\n",
    "def P(word, N=sum(WORDS.values())):\n",
    "    \"Probability of `word`.\"\n",
    "    if  word in WORDS.keys():\n",
    "        number = WORDS[word]\n",
    "    else:\n",
    "        number = 1\n",
    "    if number == 0:\n",
    "        number = 1\n",
    "    return number / N\n",
    "\n",
    "def correction(word):\n",
    "      return max(candi(word), key=P)\n",
    "\n",
    "def candi(word):\n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words):\n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcçdefgğhıijklmnoöprsştuüvyzw'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    " \n",
    "    sp     = replaceall(word,'ı','i')\n",
    "    sp2     = replaceall(word,'u','ü')\n",
    "    sp3    = replaceall(word,'o','ö')\n",
    "    sp4     = replaceall(word,'g','ğ')\n",
    "    sp5     = replaceall(word,'c','ç')\n",
    "    sp6     = replaceall(word,'s','ş')\n",
    "    sp7     = replaceall(word,'i','ı')\n",
    "    sp8     = replaceall(word,'ö','o')\n",
    "    sp9     = replaceall(word,'ş','s')\n",
    "    sp10     = replaceall(word,'ğ','g')\n",
    "    sp11     = replaceall(word,'ç','c')\n",
    "    sp12     = replaceall(word,'ü','u')\n",
    "    specials=[]\n",
    "    specials.extend(sp)\n",
    "    specials.extend(sp2)\n",
    "    specials.extend(sp3)\n",
    "    specials.extend(sp4)\n",
    "    specials.extend(sp5)\n",
    "    specials.extend(sp6)\n",
    "    specials.extend(sp7)\n",
    "    specials.extend(sp8)\n",
    "    specials.extend(sp9)\n",
    "    specials.extend(sp10)\n",
    "    specials.extend(sp11)\n",
    "    specials.extend(sp12)\n",
    "    return set(deletes+transposes+replaces+inserts+specials)\n",
    "\n",
    "def edits2(word):\n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "def print_diff(word, s):\n",
    "    if not word == s:\n",
    "        print(word + \" --> \" + s)\n",
    "counter = 0\n",
    "\n",
    "\n",
    "def lemmatizer(word,texts):\n",
    "        wordList=[]\n",
    "        wordList = re.sub(\"[^\\w]\", \" \",  texts).split()\n",
    "       \n",
    "        pos=wordList.index(word)\n",
    "     \n",
    "        sakin=''\n",
    "        word=correction(word)\n",
    "        \n",
    "        if len(wordList)-pos>3 and pos>2:\n",
    "            for i, kelime in enumerate(wordList[pos-3:pos+4]):\n",
    "                sakin=sakin+correction(kelime)+' '\n",
    "        elif pos<=2 and len(wordList)-pos>5:\n",
    "            for i, kelime in enumerate(wordList[pos:pos+5]):\n",
    "                sakin=sakin+correction(kelime)+' '\n",
    "        elif pos<=2 and len(wordList)-pos<=5:\n",
    "            for i, kelime in enumerate(wordList[pos:len(wordList)]):\n",
    "                sakin=sakin+correction(kelime)+' '\n",
    "        elif len(wordList)-pos<1 and pos>3:\n",
    "            for i, kelime in enumerate(wordList[pos-3:len(wordList)]):\n",
    "                sakin=sakin+correction(kelime)+' '\n",
    "        elif len(wordList)<3:\n",
    "            for i, kelime in enumerate(wordList):\n",
    "                sakin=sakin+correction(kelime)+' '\n",
    "        else:\n",
    "             for i, kelime in enumerate(wordList):\n",
    "                sakin=sakin+correction(kelime)+' '\n",
    "        results = morphology.analyze(word)\n",
    "        lemma=[]\n",
    "        form=[]\n",
    "        l=[]\n",
    "        m=[]\n",
    "        for i, result in enumerate(results):\n",
    "            form.append(str(result.formatLong()))\n",
    "            lemma.append(result.getLemmas()[0])\n",
    "        if len(lemma)>1:\n",
    "                analysis = morphology.analyzeSentence(sakin)\n",
    "                results = morphology.disambiguate(sakin, analysis).bestAnalysis()\n",
    "                for i, result in enumerate(results):\n",
    "                        l.append(result.getLemmas()[0])\n",
    "                        m.append(result.formatLong())\n",
    "                for i in range(len(m)):\n",
    "                    for j in range(len(form)):\n",
    "                        if m[i]==form[j]:\n",
    "                            lema=lemma[j]\n",
    "                            if lema=='değil':\n",
    "                                return 'değil'\n",
    "                            if 'Neg' in form[j] or 'WithoutHavingDoneSo' in form[j] or 'Unable' in form[j]:\n",
    "                                if checkOpennes(word):\n",
    "                                    return lema+'me'\n",
    "                                else:\n",
    "                                    return lema+'ma'\n",
    "                            if 'Without' in form[j]:\n",
    "                                if PresentCheck(word)=='ei':\n",
    "                                    return lema+'siz'\n",
    "                                elif PresentCheck(word)=='aı':\n",
    "                                    return lema+'sız'\n",
    "                                elif PresentCheck(word)=='uo':\n",
    "                                    return lema+'suz'\n",
    "                                else:\n",
    "                                    return lema+'süz'\n",
    "                            if 'With' in form[j]:\n",
    "                                if PresentCheck(word)=='ei':\n",
    "                                    return lema+'li'\n",
    "                                elif PresentCheck(word)=='aı':\n",
    "                                    return lema+'lı'\n",
    "                                elif PresentCheck(word)=='uo':\n",
    "                                    return lema+'lu'\n",
    "                                else:\n",
    "                                    return lema+'lü'\n",
    "                            else:\n",
    "                                return lema\n",
    "                    else:\n",
    "                        continue\n",
    "        elif len(lemma)==1:\n",
    "            if lemma[0]=='değil':\n",
    "                return lemma[0]\n",
    "            if 'Neg' in form[0] or 'WithoutHavingDoneSo' in form[0] or 'Unable' in form[0]:\n",
    "                 if checkOpennes(word):\n",
    "                    return lemma[0]+'me'\n",
    "                 else:\n",
    "                    return lemma[0]+'ma'\n",
    "            elif 'Without' in form[0]:\n",
    "                if PresentCheck(word)=='ei':\n",
    "                    return lemma[0]+'siz'\n",
    "                elif PresentCheck(word)=='aı':\n",
    "                    return lemma[0]+'sız'\n",
    "                elif PresentCheck(word)=='uo':\n",
    "                    return lemma[0]+'suz'\n",
    "                else:\n",
    "                    return lemma[0]+'süz'\n",
    "            elif 'With' in form[0]:\n",
    "                if PresentCheck(word)=='ei':\n",
    "                    return lemma[0]+'li'\n",
    "                elif PresentCheck(word)=='aı':\n",
    "                    return lemma[0]+'lı'\n",
    "                elif PresentCheck(word)=='uo':\n",
    "                    return lemma[0]+'lu'\n",
    "                else:\n",
    "                    return lemma[0]+'lü'\n",
    "            else:\n",
    "                return lemma[0]\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open(r\"utilities/bizim_sozluk.json\", \"r\")\n",
    "str_sozluk=f.read()\n",
    "str_sozluk=str_sozluk.replace(\"\\'\", \"\\\"\")\n",
    "bizim_sozluk = json.loads(str_sozluk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text_list=text.split()\n",
    "    for i, word in enumerate(text_list):\n",
    "        if (word in bizim_sozluk):\n",
    "            text_list[i] =bizim_sozluk[word]\n",
    "        else:\n",
    "            text_list[i] = lemmatizer(word,text)\n",
    "            bizim_sozluk[word]= text_list[i]\n",
    "    return ' '.join(''.join(elems) for elems in text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\eda\\anaconda3\\lib\\site-packages (4.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\eda\\anaconda3\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\eda\\anaconda3\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\eda\\anaconda3\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\eda\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in c:\\users\\eda\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\eda\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\eda\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\eda\\anaconda3\\lib\\site-packages (from transformers) (4.49.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\eda\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: six in c:\\users\\eda\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\eda\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\eda\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\eda\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eda\\anaconda3\\lib\\site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eda\\anaconda3\\lib\\site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\eda\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"savasy/bert-base-turkish-ner-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"savasy/bert-base-turkish-ner-cased\")\n",
    "ner_model=pipeline('ner', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "silinmeyecekler=[\"arı\",\"peron\",\"durak\",\"ring\",\"roger\",\"rogar\",\"sivrisinek\",\"sivri\",\"karasinek\"]\n",
    "silinmeyecekler=set(silinmeyecekler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open(r\"utilities/ner.json\", \"r\")\n",
    "str_sozluk=f.read()\n",
    "str_sozluk=str_sozluk.replace(\"\\'\", \"\\\"\")\n",
    "ner_oluşan_sözlük = json.loads(str_sozluk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_bul(text):\n",
    "    kelime_dizisi=text.split()\n",
    "    son_hal=\"\"\n",
    "    for i in kelime_dizisi:\n",
    "        if (i in silinmeyecekler):\n",
    "            son_hal+=\" \"+i\n",
    "        elif (i in ner_oluşan_sözlük):\n",
    "            if ( ner_oluşan_sözlük[i]>=0.94):\n",
    "                continue\n",
    "            elif (ner_oluşan_sözlük[i]<0.94):\n",
    "                son_hal+=\" \"+i\n",
    "        else:\n",
    "            result=ner_model(i)\n",
    "            if (result!=[]):\n",
    "                if (result[0][\"score\"]>=0.94):\n",
    "                    ner_oluşan_sözlük[i]=1.0\n",
    "                    continue\n",
    "                else:\n",
    "                    son_hal+=\" \"+i\n",
    "                    ner_oluşan_sözlük[i]=0.0\n",
    "            else:\n",
    "                son_hal+=\" \"+i\n",
    "                ner_oluşan_sözlük[i]=0.00\n",
    "\n",
    "    return son_hal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# İSİM SİL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(r\"utilities/isim.txt\", \"r\",encoding=\"utf8\")\n",
    "deneme=f.read()\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "deneme=deneme.replace(\"\\n\", \" \")\n",
    "isim_listesi=deneme.split(\" \")\n",
    "set_isim_list=set(isim_listesi)\n",
    "isimler=[]\n",
    "for i in set_isim_list:\n",
    "    isimler.append(i.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isim_sil(text):\n",
    "    texts=text.split()\n",
    "    sonhal=\"\"\n",
    "    for i in texts:\n",
    "        if (i in isimler):\n",
    "            continue\n",
    "        else:\n",
    "            sonhal+=\" \"+i\n",
    "    return sonhal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open(r\"utilities/bizim_sozluk.json\", \"r\")\n",
    "zemb_dict=f.read()\n",
    "zemb_dict=zemb_dict.replace(\"\\'\", \"\\\"\")\n",
    "zemb_dict = json.loads(zemb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open(r\"utilities/master.txt\", \"r\")\n",
    "dictionary=f.read()\n",
    "dictionary = dictionary.strip('][').split(', ') \n",
    "for i in range(len(dictionary)):\n",
    "    dictionary[i]=dictionary[i].replace(\"\\'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda_splitword(word):\n",
    "\n",
    "    for x in range(3,len(word)-2): ## sağlıkduyar -> sağlık duyar\n",
    "        if (word[0:x] in dictionary and word[x:len(word)] in dictionary): \n",
    "            return word[0:x]+\" \"+word[x:len(word)]\n",
    "    for x in range(3,len(word)-2):  ## duyarsağlik -> duyar sağlık\n",
    "        if (word[0:x] in dictionary and word[x:len(word)] not in dictionary):\n",
    "            if ( word[x:len(word)] in zemb_dict):\n",
    "                return word[0:x]+\" \"+zemb_dict[word[x:len(word)]]\n",
    "    for x in range(3,len(word)-2):  ##ssağlikduyar -> sağlık duyar\n",
    "        if (word[0:x] not in dictionary and word[x:len(word)]  in dictionary):\n",
    "            if ( word[0:x] in zemb_dict):\n",
    "                return zemb_dict[word[0:x]]+\" \"+word[x:len(word)]\n",
    "    ## bundan sonra zemberek kontrolleri\n",
    "    for x in range(3,len(word)-2): ### pekiiiii 2 side zemberekde var mı??  ## sağlikbenım -> sağlık ben\n",
    "        if (word[0:x]  in zemb_dict and word[x:len(word)]  in zemb_dict):\n",
    "            return zemb_dict[word[0:x]]+\" \"+zemb_dict[word[x:len(word)]]\n",
    "    for x in range(3,len(word)-2): ### sağlikbenımm -> sağlık benımm\n",
    "        if (word[0:x]  in zemb_dict and word[x:len(word)] not in zemb_dict):\n",
    "            return zemb_dict[word[0:x]]+\" \"+word[x:len(word)]\n",
    "    for x in range(3,len(word)-2): ### benımsağlik -> benımm sağlık\n",
    "        if (word[0:x] not in zemb_dict and word[x:len(word)]  in zemb_dict):\n",
    "            return word[0:x]+\" \"+zemb_dict[word[x:len(word)]]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "silinmeyecekler=[\"ağ\", \"üz\", \"iz\",\"de\", \"eş\", \"of\", \"ön\",  \"ip\", \"üs\", \"eğ\", \"su\",  \"tv\", \"in\", \"uy\", \"en\", \"es\",  \"il\", \"öd\",\n",
    "                 \"ör\", \"as\", \"ev\", \"af\",  \"el\",  \"uç\", \"ov\",  \"ne\", \"az\", \"ay\", \"öt\",  \"al\", \"oy\", \"iş\", \"öv\",  \"öz\", \"öl\", \"ez\",\n",
    "                 \"an\", \"er\", \"eh\", \"ol\", \"it\", \"av\", \"ye\",\"aş\",\"uf\",\"tl\",\"he\", \"at\",\"et\",\"öp\",\"ad\",\"ey\",\"ak\",\"oh\",\"ün\",\"un\",\"ah\", \"aç\", \"ek\",\"iç\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bul_2_sil(text):\n",
    "    texts=text.split()\n",
    "    yenii=\"\"\n",
    "    for i in range(len(texts)):\n",
    "        if (len(texts[i])==2):\n",
    "            if (texts[i] in silinmeyecekler):\n",
    "                yenii+=\" \"+texts[i]\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            yenii+=\" \"+texts[i]\n",
    "    return yenii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_preprocessing(input):\n",
    "    ##input_text=bul_2_sil(stopwords_sil(sil_1(isim_sil(ner_bul(clean(stopwords_sil(sil_1(replace_a(preProcessing(replaceemoticon(input)))))))))))\n",
    "    input_text=replaceemoticon(input)\n",
    "    #print(input_text,\" 1\")\n",
    "    input_text=preProcessing(input_text)\n",
    "    #print(input_text,\" 2\")\n",
    "    input_text=replace_a(input_text)\n",
    "    #print(input_text,\" 3\")\n",
    "    input_text=sil_1(input_text)\n",
    "    #print(input_text,\" 4\")\n",
    "    input_text=stopwords_sil(input_text)\n",
    "    #print(input_text,\" 5\")\n",
    "    input_text=clean(input_text)\n",
    "    #print(input_text,\" 6\")\n",
    "    input_text=ner_bul(input_text)\n",
    "    #print(input_text,\" 7\")\n",
    "    input_text=isim_sil(input_text)\n",
    "    #print(input_text,\" 8\")\n",
    "    input_text=sil_1(input_text)\n",
    "    #print(input_text,\" 9\")\n",
    "    input_text=stopwords_sil(input_text)\n",
    "    #print(input_text,\" 10\")\n",
    "    input_text=bul_2_sil(input_text)\n",
    "    #print(input_text,\" 11\")\n",
    "    \n",
    "    return input_text\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' sokak ilaçla talep et sinek var hamam böcek var ben adım'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preprocessing(\"55. sokakta ilaçlama talqb edıorum çok sinek var hamam boceği var /n/n/n İstanbul\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "data=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = data[[\"clean_ner\", \"aciliyet\"]]\n",
    "\n",
    "# aciliyet == 1 is the most important (urgent) priority, the rest is there to differenciate 1 from others \n",
    "filter0 = data[\"aciliyet\"] == 0 #priority 0\n",
    "filter1 = data[\"aciliyet\"] == 1 #priority 1\n",
    "filter2 = data[\"aciliyet\"] == 2 \n",
    "filter3 = data[\"aciliyet\"] == 3\n",
    "\n",
    "# 2100 data of priority 1, 700 each from the rest to create balanced sample (700x3 = 2100)\n",
    "data0 = data[filter0].iloc[0:700]\n",
    "data1 = data[filter1].iloc[0:2100] # we need the most of the data from priority 1\n",
    "data2 = data[filter2].iloc[0:700]\n",
    "data3 = data[filter3].iloc[0:700]\n",
    "\n",
    "# list all of the sampled data, then concatonate to create a dataframe from them\n",
    "frames = [data0, data1,data2,data3] \n",
    "data = pd.concat(frames, ignore_index= True)\n",
    "\n",
    "# turn all labels other than aciliyet 1 into aciliyet 2 in order to create 2 groups \n",
    "data['aciliyet'] = np.where(data['aciliyet'] == 1, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "#train and tfidf vectorize messages\n",
    "data[\"Tfidf_Vec\"] = list(tfidf_vectorizer.fit_transform(data[\"clean_ner\"]).toarray())\n",
    "\n",
    "x = data[\"Tfidf_Vec\"]\n",
    "y = data[\"aciliyet\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, #80/20 train/test\n",
    "                                                    stratify=y, #fairly distribute proportion of labels\n",
    "                                                    random_state=123, #remember the randomazition \n",
    "                                                    shuffle=True) #shuffle the data\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(list(X_train), y_train)     #train the model\n",
    "y_pred = classifier.predict(list(X_test))  #make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_input(text):    \n",
    "    mesaj=all_preprocessing(text)\n",
    "    mesajtfidf = tfidf_vectorizer.transform([mesaj]).toarray()  #modele sokabilmek için tfidf vektörüne çevir\n",
    "    mesajtfidf                                       #tfidf vektörü böyle çağrılıyor\n",
    "    #print(classifier.predict(mesajtfidf)[0])\n",
    "    if classifier.predict(mesajtfidf)[0]==1:\n",
    "        return 1\n",
    "    elif classifier.predict(mesajtfidf)[0]==2:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_input(\"yangın çıktı\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: You must give at least one requirement to install (see \"pip help install\")\n"
     ]
    }
   ],
   "source": [
    "pip install -I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acil1_color=\"#FF0008\"\n",
    "acil2_color=\"#FF575C\"\n",
    "acil3_color=\"#FF9397\"\n",
    "\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "from tkinter import messagebox \n",
    "from PIL import Image, ImageTk\n",
    "import PIL.Image\n",
    "import PIL.ImageTk\n",
    "\n",
    "root=Tk()\n",
    "root.title(\" Priorist\")\n",
    "root.geometry(\"1200x600\")\n",
    "##root.config(background='black')\n",
    "## create a main frame\n",
    "main_frame=Frame(root)\n",
    "main_frame.pack(fill=BOTH,expand=1)\n",
    "#create a canvas\n",
    "my_canvas=Canvas(main_frame)\n",
    "my_canvas.pack(side=LEFT,fill=BOTH,expand=1)\n",
    "## add a scrollbar to the canvas\n",
    "my_scrollbar=ttk.Scrollbar(main_frame,orient=VERTICAL,command=my_canvas.yview)\n",
    "my_scrollbar.pack(side=RIGHT,fill=Y)\n",
    "## configura the canvas\n",
    "my_canvas.configure(yscrollcommand=my_scrollbar.set)\n",
    "my_canvas.bind('<Configure>',lambda e:my_canvas.configure(scrollregion=my_canvas.bbox(\"all\")))\n",
    "## create another frame inside the canvas\n",
    "second_frame=Frame(my_canvas)\n",
    "third_frame=Frame(my_canvas)\n",
    "my_canvas.create_window((0,0),window=second_frame,anchor=\"nw\")\n",
    "\n",
    "l1_count=0\n",
    "l2_count=0\n",
    "vatandas_mesajı=\"\"\n",
    "def submit():\n",
    "    global l1_count\n",
    "    global l2_count\n",
    "    global l3_count\n",
    "    vatandas_mesajı=message.get()\n",
    "    print(\"Mesaj: \" + vatandas_mesajı)\n",
    "    model_cevabı=predict_input(vatandas_mesajı)\n",
    "    print(\"Model cevabı: \",model_cevabı)\n",
    "    vatandas_mesajı=\" \"+vatandas_mesajı\n",
    "    if(model_cevabı==1):\n",
    "        Lb1.insert(l1_count, vatandas_mesajı)\n",
    "        l1_count=l1_count+1\n",
    "    elif(model_cevabı==2):\n",
    "        Lb2.insert(l2_count, vatandas_mesajı)\n",
    "        l2_count=l2_count+1\n",
    "    message.delete(0, END)\n",
    "\n",
    "def basla_fonk():\n",
    "    global id\n",
    "    global basla_kontrol\n",
    "    my_text=df[\"TICKET_DESC\"].iloc[id].replace(\"\\n\",\" \")\n",
    "    text.configure(text=my_text)\n",
    "    text2.configure(text=id)\n",
    "    basla.destroy()\n",
    "    acil1.configure(bg=acil1_color)\n",
    "    acil2.configure(bg=acil2_color)\n",
    "    acil3.configure(bg=acil3_color)\n",
    "    acil_degil.configure(bg=notr_color)\n",
    "    negatif.configure(bg=neg_color)\n",
    "    notr.configure(bg=notr_color)\n",
    "    pos.configure(bg=pos_color)\n",
    "    sıradaki.configure(bg=sıradaki_color)\n",
    "    kaydet.pack(side=LEFT,padx=50)\n",
    "    basla_kontrol=1\n",
    "\n",
    "text = Label(second_frame,text = \"  \",wraplength=600)\n",
    "text.grid(row = 0, column = 1,padx=20,pady=5,sticky='nsew')\n",
    "\n",
    "\n",
    "text = Label(second_frame,text = \"Acil\",bg=acil1_color)\n",
    "text.grid(row = 0, rowspan=1,column = 0,padx=20,sticky='nsew')\n",
    "text = Label(second_frame,text = \"Acil Değil\",bg=\"grey\")\n",
    "text.grid(row = 0,rowspan=1, column = 3,padx=20,sticky='nsew')\n",
    "\n",
    "Lb1 = Listbox(second_frame,height='23',width='57',highlightbackground=acil1_color)\n",
    "Lb1.grid(row = 1,rowspan=6, column = 0,padx=20,pady=5,sticky='nsew')\n",
    "\n",
    "Lb2 = Listbox(second_frame, height='23',width='57',highlightbackground=\"grey\")\n",
    "Lb2.grid(row = 1, rowspan=6,column = 3,padx=20,pady=5,sticky='nsew')\n",
    "\n",
    "#logo = Label(second_frame,text = \"Priorist\",font=(\"Helvetica\", 32))\n",
    "#logo.grid(row = 1, column = 1,padx=20,pady=5,sticky='nsew')\n",
    "\n",
    "im = PIL.Image.open(\"utilities/prior2.png\")\n",
    "photo = PIL.ImageTk.PhotoImage(im)\n",
    "label = Label(second_frame, image=photo)\n",
    "label.image = photo  # keep a reference!\n",
    "label.grid(row = 0, column = 1,rowspan=2)\n",
    "\n",
    "message = Entry(second_frame,textvariable = vatandas_mesajı, font=('Helvetica',10,'normal'),highlightbackground=\"green\", highlightcolor=\"green\", highlightthickness=1, bd=0)\n",
    "message.grid(row=2,rowspan = 2, column = 1, padx=10,\n",
    "               pady=10,\n",
    "               ipadx=100,\n",
    "               ipady=40)\n",
    "message.insert(0, ' Mesaj giriniz..')\n",
    "\n",
    "sub_btn=Button(second_frame,text = 'Gönder', command = submit,bg=\"lightgreen\",height=\"2\" )\n",
    "sub_btn.grid(row=4,rowspan=4,column = 1,padx=10,pady=60,sticky='nsew')\n",
    "\n",
    "\n",
    "second_frame.pack(expand=1)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}